# twitter_operator - Operator to collect tweets from the Twitter API.
#
# Author: Duarte Junior <duarte.jr105@gmail.com>
#
# Licensed to GNU General Public License under a Contributor Agreement.
import json
import time
import pandas as pd
from os.path import join
from pathlib import Path
from datetime import datetime, timedelta
from hooks.twitter_hook import TwitterHook
from airflow.models import BaseOperator, DAG
from airflow.utils.decorators import apply_defaults


class TwitterOperator(BaseOperator):
    """
    This operator creates a new instance of the TwitterHook class which is 
    responsible for establishing a connection with the Twitter API and 
    collecting the tweets. It uses the parameters provided to create the API URL
    and make the request to the API, the tweets collected will be stored in a 
    Spark dataframe for further processing.
    """

    template_fields = ["query", "file_path", "start_time", "end_time", "country"]

    @apply_defaults
    def __init__(self, query: str, file_path: str, conn_id=None, start_time=None,
                 end_time=None, country=None, *args, **kwargs):
        """
        This function create a new TwitterOperator instance.

        Args:
            query (str): The text that the Twitter API will search in the tweets.
            file_path (str): Path of the destination folder
            conn_id (optional): Parameters to stablish a connection with the 
                                Twitter API. Defaults to None.
            start_time (str, optional): The start date to search the tweets.
                                        Defaults to None.
            end_time (str, optional): The end date to search the tweets. 
                                      Defaults to None.
            country (str, optional): The name of the country of origin of the 
                                     tweets. Defaults to None.
        """
        super().__init__(*args, **kwargs)
        self.query = query
        self.file_path = file_path  
        self.conn_id = conn_id
        self.start_time = start_time
        self.end_time = end_time
        self.country = country
    
    def create_parent_folder(self):
        """
        This function creates the destination folder if it does not exist.
        """
        Path(Path(self.file_path).parent).mkdir(parents=True, exist_ok=True)

    def execute(self):
        """
        This function axecutes all the tasks needed to collect and storage the
        tweets.
        """
        # It creates a new TwitterHook
        hook = TwitterHook(query = self.query,
                           conn_id = self.conn_id,
                           start_time = self.start_time,
                           end_time = self.end_time,
                           country = self.country)
        self.create_parent_folder() # It creates the destination file.
        
        # It opens the output file do insert data into it.
        with open(self.file_path, "w") as output_file:
            # It interates in each row generated by the Twitter API.
            for pg in hook.run(): 
                json.dump(pg, output_file, ensure_ascii=False)
                output_file.write("\n")
                # The next line of code implements a sleep function that will 
                # pause the execution of the program for 5 seconds after each 
                # request to the Twitter API. This is necessary to avoid 
                # exceeding the API rate limit and ensure that the requests are 
                # sent at a controlled rate.
                time.sleep(5)


if __name__ == "__main__":
    # This section is an example of how to use the functions related to 
    # collecting Tweets in an Airflow DAG. It will demonstrate how these 
    # functions will be called and executed when the Airflow pipeline is run. 
    # The specific steps and commands that need to be executed will depend on 
    # the implementation of the functions and the desired behavior of the 
    # Airflow DAG.
    start_date = '2019-1-1' # Start date to collect the tweets.
    end_date = '2021-12-31' # End date to collect the tweets.
    TIMESTAMP_FORMAT = "%Y-%m-%dT%H:%M:%S.00Z"
    
    # Iterates in each date
    for st_dt in pd.date_range(start=start_date, end=end_date):
        ds_date = datetime.strftime(st_dt, "%Y-%m-%d")
        ds_date_nodash = datetime.strftime(st_dt, "%Y%m%d")
        print(st_dt)
        
        # Iterates in each country
        for country in ["ES", 'EC', 'CL', 'MX', 'AR', 'BR']:
            print(country)
            
            # it tests the Twitter DAG
            with DAG(dag_id="TwitterTest", start_date=st_dt) as dag:
                to = TwitterOperator(query = "covid",
                                     file_path = join("/mnt/d/bootcamp-covid/datalake/bronze",
                                                      "twitter_covid",
                                                      country,
                                                      f"extract_date={ds_date}",
                                                      f"CovidTweets_{country}_{ds_date_nodash}.json"),
                                     task_id = "test_run",
                                     start_time = datetime.strftime(st_dt,
                                                                    TIMESTAMP_FORMAT),
                                     end_time = datetime.strftime(st_dt + timedelta(days=1),
                                                                  TIMESTAMP_FORMAT),
                                     country=country)
                
                to.execute(None)
                time.sleep(10)
            time.sleep(5)
        time.sleep(10)
